ğŸ“˜ æ ‘è“æ´¾ 5ï¼ˆ8GBï¼‰éƒ¨ç½² Qwen2.5-7B æ¨¡å‹å®æˆ˜æ–‡æ¡£

Deployment Report: Running Qwen2.5-7B on Raspberry Pi 5 (8GB) with llama.cpp

1ï¸âƒ£ éƒ¨ç½²ç›®æ ‡ | Deployment Goal
ä¸­æ–‡

åœ¨ æ ‘è“æ´¾ 5ï¼ˆ8GB å†…å­˜ï¼‰ ä¸Šæœ¬åœ°éƒ¨ç½²ä¸€ä¸ªå¯ç”¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ”¯æŒï¼š

æœ¬åœ° CLI äº¤äº’

åç»­å¯æ‰©å±•ä¸º HTTP API

å¯è¢« å¦ä¸€å° Mac / PC è°ƒç”¨

English

Deploy a local large language model on Raspberry Pi 5 (8GB RAM) that:

Runs via CLI

Can be exposed as a local HTTP API

Can be accessed remotely (e.g. from a Mac)

2ï¸âƒ£ æœ€ç»ˆé€‰æ‹©çš„æŠ€æœ¯æ–¹æ¡ˆ | Final Chosen Solution
âœ… å®é™…éƒ¨ç½²æ–¹æ¡ˆï¼ˆæœ€ç»ˆæˆåŠŸï¼‰
é¡¹ç›®	é€‰æ‹©
æ¨ç†æ¡†æ¶	llama.cpp
æ¨¡å‹æ ¼å¼	GGUFï¼ˆé‡åŒ–ï¼‰
æ¨¡å‹	Qwen2.5-7B-Instruct
é‡åŒ–	Q4_0ï¼ˆCPU å‹å¥½ï¼‰
è¿è¡Œæ–¹å¼	CLI â†’ å¯æ‰©å±•åˆ° API
âŒ æœªé‡‡ç”¨æ–¹æ¡ˆï¼ˆåŸå› ï¼‰
æ–¹æ¡ˆ	é—®é¢˜
Ollama	ç½‘ç»œå—é™ï¼Œå®‰è£…è„šæœ¬è¢«é‡ç½®
åŸå§‹ FP16 æ¨¡å‹	å†…å­˜ä¸å¤Ÿ
å•æ–‡ä»¶ GGUF	HuggingFace å®é™…æä¾›çš„æ˜¯åˆ†ç‰‡
3ï¸âƒ£ å®Œæ•´éƒ¨ç½²æµç¨‹ | Full Deployment Workflow
Step 1ï¼šå‡†å¤‡ç¯å¢ƒ

Prepare the environment

sudo apt update
sudo apt install -y git cmake build-essential


âš ï¸ æ³¨æ„
llama.cpp å·² ä¸å†æ”¯æŒ makeï¼Œå¿…é¡»ä½¿ç”¨ CMake

Step 2ï¼šè·å– llama.cpp æºç 

Get llama.cpp source code

cd ~
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

Step 3ï¼šä½¿ç”¨ CMake ç¼–è¯‘

Build with CMake (required)

mkdir build
cd build
cmake ..
cmake --build . --config Release -j$(nproc)

âš ï¸ é‡åˆ°çš„é—®é¢˜

âŒ make -j$(nproc) æŠ¥é”™

âŒ æç¤º Build system changed: replaced by CMake

âœ… è§£å†³æ–¹å¼

æ”¹ç”¨ cmake + cmake --build

é—®é¢˜æœ¬èº«ä¸æ˜¯é”™è¯¯ï¼Œè€Œæ˜¯æ„å»ºç³»ç»Ÿå‡çº§æç¤º

Step 4ï¼šæ¨¡å‹æƒé‡å‡†å¤‡ï¼ˆå…³é”®ï¼‰

Prepare model weights (CRITICAL)

â— å¸¸è§è¯¯è§£

llama.cpp ä¸ä¼šè‡ªå¸¦æ¨¡å‹æƒé‡
models ç›®å½•é‡Œåªæœ‰ vocab æ–‡ä»¶ï¼Œä¸æ˜¯æ¨¡å‹

å®é™…ä¸‹è½½åˆ°çš„æ–‡ä»¶ï¼ˆHuggingFaceï¼‰
qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf
qwen2.5-7b-instruct-q4_0-00002-of-00002.gguf


è¿™æ˜¯ åˆ†ç‰‡ GGUF æ¨¡å‹ï¼Œä¸æ˜¯é”™è¯¯ã€‚

å»ºè®®ç›®å½•ç»“æ„
/home/zhang1083/models/
â”œâ”€â”€ qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf
â””â”€â”€ qwen2.5-7b-instruct-q4_0-00002-of-00002.gguf

Step 5ï¼šç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆCLIï¼‰

Run the model via CLI

cd ~/llama.cpp/build

./bin/llama-cli \
  -m /home/zhang1083/models/qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf \
  -t 4 \
  -c 2048 \
  -n 128

4ï¸âƒ£ å…³é”®é—®é¢˜ä¸è§£å†³æ€»ç»“ | Problems & Solutions
âŒ é—®é¢˜ 1ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶
failed to open GGUF file (æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•)


åŸå› 

ä½¿ç”¨äº†ä¸å­˜åœ¨çš„è·¯å¾„

ä½¿ç”¨äº†â€œæƒ³è±¡ä¸­çš„åˆå¹¶æ–‡ä»¶åâ€

è§£å†³

ä½¿ç”¨ find æŸ¥æ‰¾çœŸå® gguf

ä½¿ç”¨å®é™…å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„

âŒ é—®é¢˜ 2ï¼šinvalid split file name
invalid split file name: Qwen2.5-7B-Instruct-Q4_0.gguf


åŸå› 

åˆ†ç‰‡æ¨¡å‹ä¸èƒ½ç”¨â€œå‡æƒ³åˆå¹¶åâ€

llama.cpp éœ€è¦ ç¬¬ä¸€ä¸ªåˆ†ç‰‡æ–‡ä»¶

æ­£ç¡®æ–¹å¼

-m qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf


llama.cpp ä¼š è‡ªåŠ¨åŠ è½½åç»­åˆ†ç‰‡

âŒ é—®é¢˜ 3ï¼šè¯¯ä»¥ä¸º vocab å°±æ˜¯æ¨¡å‹
ggml-vocab-qwen2.gguf


åŸå› 

vocab æ–‡ä»¶ â‰  æ¨¡å‹æƒé‡

æƒé‡ä½“ç§¯é€šå¸¸æ˜¯ å‡ ä¸ª GB

è§£å†³

æ˜ç¡®åŒºåˆ† vocab / weights

æƒé‡å¿…é¡»å•ç‹¬ä¸‹è½½

5ï¸âƒ£ æˆåŠŸæ ‡å¿— | Success Indicators
å½“ä½ çœ‹åˆ°è¿™äº›ï¼Œè¯´æ˜éƒ¨ç½²æˆåŠŸ âœ…

llama.cpp ASCII Logo

available commands

è¿›å…¥äº¤äº’å¼è¾“å…¥

æ— ä»»ä½• load error

available commands:
  /exit
  /regen
  /clear

6ï¸âƒ£ å½“å‰çŠ¶æ€æ€»ç»“ | Final Status
ä¸­æ–‡

âœ… æ ‘è“æ´¾ 5 æˆåŠŸè¿è¡Œ Qwen2.5-7B

âœ… ä½¿ç”¨ CPU + Q4 é‡åŒ–

âœ… CLI å·²å¯äº¤äº’

ğŸ”œ å¯æ‰©å±•ä¸º HTTP API

ğŸ”œ å¯è¢« Mac / PC è¿œç¨‹è°ƒç”¨

English

Qwen2.5-7B successfully runs on Raspberry Pi 5

CPU-only with Q4 quantization

CLI interaction works

Ready for API exposure and remote access
